Notebooks and scripts for fine-tuning a Masked Language Model (MLM) like BERT (base or large) by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model.

The [requirements.txt](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/requirements.txt) gives you the datasets version needed and here is the list of the libraries versions used in a virtual environment when we ran successfully notebooks and scripts of this folder:

- python: 3.8.10 (default, Jun  4 2021, 15:09:15) 
- Pytorch: 1.9.0
- adapter-transformers: 2.0.1
- HF transformers: 4.5.1
- tokenizers: 0.10.3
- datasets: 1.8.0

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning BERT (base or large) on a Question-Answering task by using the library adapter-transformers (script version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: 02/07/2021\n",
    "- **Blog post**: []()\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [question-answering with adapters](https://github.com/piegu/language-models/tree/master/adapters/question-answering/)\n",
    "- **Link to the adapters in the AdapterHub**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) for a QA task by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are about 3% of the MLM model one**. More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a question answering task, which is the task of extracting the answer to a question from a given context. We will use the library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers) and see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it.\n",
    "\n",
    "![Widget inference representing the QA task](images/question_answering_adapter.png)\n",
    "\n",
    "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) on the QA task with any QA dataset** (we use here the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [04_Cross_Lingual_Transfer.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb) and script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) (this script was adapted from the script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) and script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) allows to evaluate the model performance against f1 metric at the end of each epoch, and not against validation loss as done in the notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb). This is very important as we consider the metric when selecting a model, not the loss. Therefore, we decided to launch this script inside this notebook (by simulating terminal command line) instead of running code in cells as done in the notebook of HF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More, we updated the script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) to [run_qa_adapter.py](https://github.com/piegu/language-models/blob/master/adapters/question-answering/run_qa_adapter.py) with the following changes:\n",
    "- **EarlyStopping** by selecting the model with the highest eval f1 (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))\n",
    "- **Stack method** for the lang and task adapters when a lang adapter is loaded ([doc](https://docs.adapterhub.ml/adapter_composition.html?highlight=stack#stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.0.1\n",
      "HF transformers: 4.5.1\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.0.1\n",
    "# transformers: 4.5.1\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symbolic links to the folder with the scripts to run or download them in the same folder of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ln -s ~/adapter-transformers/examples/question-answering/run_qa_adapter.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/trainer_qa.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/utils_qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\"\n",
    "\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "version_2_with_negative = False # If true, some of the examples do not have an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 10.\n",
    "early_stopping_patience = 3\n",
    "\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "fp16 = True\n",
    "ds = False # If True, we use DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "metric_for_best_model = \"f1\"\n",
    "greater_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "\n",
    "# lang adapter\n",
    "with_adapters_mlm = False # if False, we do not upload an existing lang adapter\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    adapter_composition = \"stack\" # we will stack the lang and task adapters\n",
    "else:\n",
    "    adapter_composition = None\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer\n",
    "madx2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 0 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU 0\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9995'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# if you want to test the trainer, set up the following variables\n",
    "max_train_samples = 200 # None\n",
    "max_val_samples = 50 # None\n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" \n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/'\n",
    "if with_adapters_mlm:\n",
    "    outputs = outputs + 'mlm_' + str(task) + '_AdCompo' + str(adapter_composition)\n",
    "else:\n",
    "    outputs = outputs + str(task)\n",
    "outputs = outputs \\\n",
    "+ '_lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_wamlm' + str(with_adapters_mlm) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'models_outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum total input sequence length after tokenization. Sequences longer\n",
    "# than this will be truncated, sequences shorter will be padded.\n",
    "max_seq_length = 384\n",
    "\n",
    "# Whether to pad all samples to `max_seq_length`.\n",
    "# If False, will pad the samples dynamically when batching to the maximum length in the batch (which can\n",
    "# be faster on GPU but will be slower on TPU).\n",
    "pad_to_max_length = True\n",
    "    \n",
    "# The threshold used to select the null answer: if the best answer has a score that is less than\n",
    "# the score of the null answer minus this threshold, the null answer is selected for this example.\n",
    "# Only useful when `version_2_with_negative=True`.\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 128\n",
    "    \n",
    "# The total number of n-best predictions to generate when looking for an answer.\n",
    "n_best_size = 20\n",
    " \n",
    "# The maximum length of an answer that can be generated. This is needed because the start\n",
    "# and end predictions are not conditioned on one another.\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapters config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task adapter config\n",
    "adapter_config = \"pfeiffer\" # houlsby is possible, too\n",
    "adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "adapter_reduction_factor = 16\n",
    "language = 'pt' # pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_adapters_mlm:\n",
    "    \n",
    "    # hyperparameters used for fine-tuning the MLM with lang adapter\n",
    "    learning_rate_mlm = 1e-4\n",
    "    batch_size_mlm = 32\n",
    "    gradient_accumulation_steps_mlm = 1\n",
    "    adam_epsilon_mlm = 1e-6\n",
    "    num_train_epoch_mlm = 100.\n",
    "    madx2_mlm = madx2\n",
    "    ds_mlm = False\n",
    "    fp16_mlm = True\n",
    "    load_best_model_at_end_mlm = True\n",
    "    metric_for_best_model_mlm = \"loss\"\n",
    "    \n",
    "    # path to lang adapter\n",
    "    outputs_mlm = model_checkpoint.replace('/','-') + '_' + dataset_name + '/mlm' \\\n",
    "    + '_lr' + str(learning_rate_mlm) \\\n",
    "    + '_bs' + str(batch_size_mlm) \\\n",
    "    + '_GAS' + str(gradient_accumulation_steps_mlm) \\\n",
    "    + '_eps' + str(adam_epsilon_mlm) \\\n",
    "    + '_epochs' + str(num_train_epoch_mlm) \\\n",
    "    + '_madx2' + str(madx2_mlm) \\\n",
    "    + '_ds' + str(ds_mlm) \\\n",
    "    + '_fp16' + str(fp16_mlm) \\\n",
    "    + '_best' + str(load_best_model_at_end_mlm) \\\n",
    "    + '_metric' + str(metric_for_best_model_mlm)\n",
    "\n",
    "    path_to_outputs = root/'models_outputs'/outputs_mlm\n",
    "    \n",
    "    # Config of the lang adapter\n",
    "    lang_adapter_path = path_to_outputs/'adapters-mlm/'\n",
    "    \n",
    "    load_lang_adapter = lang_adapter_path\n",
    "    lang_adapter_config = str(lang_adapter_path + \"/adapter_config.json\")\n",
    "    lang_adapter_non_linearity = 'gelu'\n",
    "    lang_adapter_reduction_factor = 2\n",
    "    language_mlm = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "\n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         entry_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "#             title = row['title']\n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = paragraph['context']\n",
    "\n",
    "#                 for qa in paragraph['qas']:\n",
    "#                     entry = {}\n",
    "\n",
    "#                     qa_id = qa['id']\n",
    "#                     question = qa['question']\n",
    "#                     answers = qa['answers']\n",
    "\n",
    "#                     entry['id'] = qa_id\n",
    "#                     entry['title'] = title.strip()\n",
    "#                     entry['context'] = context.strip()\n",
    "#                     entry['question'] = question.strip()\n",
    "\n",
    "#                     answer_starts = [answer[\"answer_start\"] for answer in answers]\n",
    "#                     answer_texts = [answer[\"text\"].strip() for answer in answers]\n",
    "#                     entry['answers'] = {}\n",
    "#                     entry['answers']['answer_start'] = answer_starts\n",
    "#                     entry['answers']['text'] = answer_texts\n",
    "\n",
    "#                     entry_list.append(entry)\n",
    "\n",
    "#         reverse_entry_list = entry_list[::-1]\n",
    "\n",
    "#         # for entries with same id, keep only last one (corrected texts by the group Deep Learning Brasil)\n",
    "#         unique_ids_list = list()\n",
    "#         unique_entry_list = list()\n",
    "#         for entry in reverse_entry_list:\n",
    "#             qa_id = entry['id']\n",
    "#             if qa_id not in unique_ids_list:\n",
    "#                 unique_ids_list.append(qa_id)\n",
    "#                 unique_entry_list.append(entry)\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         new_dict = {}\n",
    "#         new_dict['data'] = unique_entry_list\n",
    "\n",
    "#         file_name = 'pt_' + str(file)\n",
    "#         with open(file_name, 'w') as json_file:\n",
    "#             json.dump(new_dict, json_file)\n",
    "            \n",
    "# %cd {root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uxSaGa_l3l-W"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    # dataset folder \n",
    "    path_to_dataset = root/'data'/dataset_name\n",
    "    \n",
    "    # paths to files\n",
    "    train_file = str(path_to_dataset/'pt_squad-train-v1.1.json')\n",
    "    validation_file = str(path_to_dataset/'pt_squad-dev-v1.1.json')\n",
    "    \n",
    "    datasets = load_dataset('json', \n",
    "                            data_files={'train': train_file, \\\n",
    "                                        'validation': validation_file, \\\n",
    "                                       }, \n",
    "                            field='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5735c47ae853931400426b64',\n",
       " 'title': 'Kathmandu',\n",
       " 'context': 'A maioria das cozinhas encontradas em Katmandu nÃ£o Ã© vegetariana. No entanto, a prÃ¡tica do vegetarianismo nÃ£o Ã© incomum, e a culinÃ¡ria vegetariana pode ser encontrada em toda a cidade. O consumo de carne bovina Ã© muito incomum e considerado tabu em muitos lugares. Buff (carne de bÃºfalo Marinho) Ã© muito comum. HÃ¡ uma forte tradiÃ§Ã£o de consumo de buffs em Katmandu, especialmente entre Newars, que nÃ£o Ã© encontrado em outras partes do Nepal. O consumo de carne de porco era considerado tabu atÃ© algumas dÃ©cadas atrÃ¡s. Devido Ã  mistura com a cozinha Kirat do leste do Nepal, a carne de porco encontrou um lugar nos pratos de Katmandu. Uma populaÃ§Ã£o marginal de hindus e muÃ§ulmanos devotos o considera tabu. Os muÃ§ulmanos proÃ­bem comer buff a partir do AlcorÃ£o, enquanto os hindus comem todas as variedades, exceto a carne de vaca, pois consideram a vaca uma deusa e sÃ­mbolo da pureza. O cafÃ© da manhÃ£ principal para moradores e visitantes Ã© principalmente Momo ou Chowmein.',\n",
       " 'question': 'De que animal vem o lustre?',\n",
       " 'answers': {'answer_start': [280], 'text': ['bÃºfalo Marinho']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572813372ca10214002d9d58</td>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>AlÃ©m da catedral, Estrasburgo abriga vÃ¡rias outras igrejas medievais que sobreviveram Ã s muitas guerras e destruiÃ§Ãµes que assolaram a cidade: a romÃ¢nica Ã‰glise Saint-Ã‰tienne, parcialmente destruÃ­da em 1944 por bombardeios aliados, parte romÃ¢nica, parte gÃ³tica, muito a grande Igreja Saint-Thomas, com seu Ã³rgÃ£o Silbermann, no qual Wolfgang Amadeus Mozart e Albert Schweitzer tocavam, o protestante da igreja gÃ³tica Saint-Pierre-le-Jeune com sua cripta que remonta ao sÃ©culo VII e seu claustro parcialmente do sÃ©culo XI, o gÃ³tico Ã‰glise Saint-Guillaume, com seus belos vitrais e mÃ³veis do inÃ­cio da RenascenÃ§a, a gÃ³tica Ã‰glise Saint-Jean, a parte gÃ³tica, a parte Art Nouveau Ã‰glise Sainte-Madeleine etc. A igreja neogÃ³tica Saint-Pierre-le-Vieux Catholique ( hÃ¡ tambÃ©m uma igreja adjacente (Saint-Pierre-le-Vieux protestante) serve de santuÃ¡rio para vÃ¡rios altares trabalhados e pintados em madeira do sÃ©culo XV, vindos de outras igrejas agora destruÃ­das e instaladas lÃ¡ para exibiÃ§Ã£o pÃºblica. Entre os numerosos edifÃ­cios medievais seculares, destaca-se o monumental Ancienne Douane (antiga alfÃ¢ndega).</td>\n",
       "      <td>Onde estÃ¡ localizado o Ã³rgÃ£o Silbermann?</td>\n",
       "      <td>{'answer_start': [276], 'text': ['Igreja Saint-Thomas']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57310cad05b4da19006bcd2b</td>\n",
       "      <td>Immaculate_Conception</td>\n",
       "      <td>A popularidade dessa representaÃ§Ã£o particular da Imaculada ConceiÃ§Ã£o se espalhou por todo o resto da Europa e desde entÃ£o continua sendo a representaÃ§Ã£o artÃ­stica mais conhecida do conceito: em um reino celestial, momentos apÃ³s sua criaÃ§Ã£o, o espÃ­rito de Maria (na forma de um jovem) olha com reverÃªncia (ou inclina a cabeÃ§a para) Deus. A lua estÃ¡ sob seus pÃ©s e um halo de doze estrelas envolve sua cabeÃ§a, possivelmente uma referÃªncia a \"uma mulher vestida de sol\" em Apocalipse 12: 1-2. Imagens adicionais podem incluir nuvens, uma luz dourada e querubins. Em algumas pinturas, os querubins estÃ£o segurando lÃ­rios e rosas, flores frequentemente associadas a Maria.</td>\n",
       "      <td>Sobre o que Maria se apÃ³ia neste sÃ­mbolo?</td>\n",
       "      <td>{'answer_start': [339], 'text': ['lua estÃ¡ sob seus pÃ©s']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572f79b4a23a5019007fc66a</td>\n",
       "      <td>Han_dynasty</td>\n",
       "      <td>As culturas bÃ¡sicas mais comuns consumidas durante o Han foram trigo, cevada, milheto, milho moÃ­do, arroz e feijÃ£o. Frutas e legumes comumente consumidos incluem castanhas, peras, ameixas, pÃªssegos, melÃµes, damascos, morangos, morangos vermelhos, jujubas, cabaÃ§a, brotos de bambu, mostarda e taro. Os animais domesticados que tambÃ©m foram comidos incluÃ­am galinhas, patos mandarim, gansos, vacas, ovelhas, porcos, camelos e cÃ£es (vÃ¡rios tipos foram criados especificamente para alimentaÃ§Ã£o, enquanto a maioria foi usada como animais de estimaÃ§Ã£o). Tartarugas e peixes foram retirados de riachos e lagos. Foram consumidos caÃ§a comum, como coruja, faisÃ£o, pega, veado-sika e perdiz de bambu chinesa. Os temperos incluÃ­am aÃ§Ãºcar, mel, sal e molho de soja. Cerveja e vinho eram consumidos regularmente.</td>\n",
       "      <td>Quais eram os cÃ£es nessa Ã©poca com maior probabilidade de serem considerados?</td>\n",
       "      <td>{'answer_start': [525], 'text': ['animais de estimaÃ§Ã£o']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5731bfcc0fdd8d15006c64fc</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>As condiÃ§Ãµes econÃ´micas comeÃ§aram a melhorar consideravelmente, apÃ³s um perÃ­odo de estagnaÃ§Ã£o, devido Ã  adoÃ§Ã£o de polÃ­ticas econÃ´micas mais liberais pelo governo, bem como ao aumento das receitas do turismo e do mercado de aÃ§Ãµes em expansÃ£o. Em seu relatÃ³rio anual, o Fundo MonetÃ¡rio Internacional (FMI) classificou o Egito como um dos principais paÃ­ses do mundo que estÃ¡ realizando reformas econÃ´micas. Algumas grandes reformas econÃ´micas empreendidas pelo governo desde 2003 incluem uma reduÃ§Ã£o drÃ¡stica de costumes e tarifas. Uma nova lei tributÃ¡ria implementada em 2005 reduziu os impostos corporativos de 40% para os atuais 20%, resultando em um aumento declarado de 100% na receita tributÃ¡ria atÃ© o ano de 2006.</td>\n",
       "      <td>Qual Ã¡rea de negÃ³cios cresceu ultimamente no Egito?</td>\n",
       "      <td>{'answer_start': [212], 'text': ['mercado de aÃ§Ãµes']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5707066b9e06ca38007e92b3</td>\n",
       "      <td>Letter_case</td>\n",
       "      <td>Em latim, foram encontrados papiros de Herculano, datados de 79 dC (quando foram destruÃ­dos), que foram escritos em cursiva romana antiga, onde as primeiras formas de letras minÃºsculas \"d\", \"h\" e \"r\", por exemplo, jÃ¡ pode ser reconhecido. Segundo o papirologista Knut Kleve, \"a teoria, portanto, de que as letras minÃºsculas foram desenvolvidas a partir dos unciais do sÃ©culo V e dos minÃºsculos carolÃ­ngios do sÃ©culo IX parece estar errada\". Havia letras maiÃºsculas e minÃºsculas, mas a diferenÃ§a entre as duas variantes era inicialmente estilÃ­stica, e nÃ£o ortogrÃ¡fica, e o sistema de escrita ainda era basicamente unicameral: um determinado documento manuscrito podia usar um estilo ou outro, mas esses nÃ£o eram mistos. As lÃ­nguas europÃ©ias, exceto o grego antigo e o latim, nÃ£o fizeram a distinÃ§Ã£o entre os casos antes de 1300. [citaÃ§Ã£o necessÃ¡rio]</td>\n",
       "      <td>Quais idiomas de continentes com poucas exceÃ§Ãµes geralmente nÃ£o utilizavam distinÃ§Ã£o entre maiÃºsculas e minÃºsculas atÃ© cerca de 1300?</td>\n",
       "      <td>{'answer_start': [730], 'text': ['europÃ©ias']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>572783acdd62a815002e9f4a</td>\n",
       "      <td>Child_labour</td>\n",
       "      <td>No Brasil, a idade mÃ­nima para o trabalho foi identificada como catorze devido a contÃ­nuas emendas constitucionais ocorridas em 1934, 1937 e 1946. No entanto, devido a uma mudanÃ§a na ditadura militar pelos anos 80, a restriÃ§Ã£o de idade mÃ­nima foi reduzida para doze anos de idade, mas foi revisada devido a relatos de condiÃ§Ãµes perigosas e perigosas de trabalho em 1988. Isso levou Ã  idade mÃ­nima sendo novamente aumentada para 14. Outro conjunto de restriÃ§Ãµes foi aprovado em 1998 que restringia os tipos de trabalho que os jovens podiam participar, como trabalhos considerados perigosos, como a operaÃ§Ã£o de equipamentos de construÃ§Ã£o ou certos tipos de trabalho na fÃ¡brica. Embora tenham sido tomadas muitas medidas para reduzir o risco e a ocorrÃªncia de trabalho infantil, ainda hÃ¡ um nÃºmero elevado de crianÃ§as e adolescentes trabalhando com menos de quatorze anos no Brasil. Somente nos anos 80 foi descoberto que quase nove milhÃµes de crianÃ§as no Brasil trabalhavam ilegalmente e nÃ£o participavam de atividades infantis tradicionais que ajudam a desenvolver importantes experiÃªncias de vida.</td>\n",
       "      <td>Qual a idade mÃ­nima para trabalhar no Brasil?</td>\n",
       "      <td>{'answer_start': [855], 'text': ['quatorze']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5726f391dd62a815002e9609</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>A prestaÃ§Ã£o de serviÃ§os de saÃºde na NigÃ©ria Ã© uma responsabilidade simultÃ¢nea dos trÃªs nÃ­veis de governo no paÃ­s e do setor privado. A NigÃ©ria vem reorganizando seu sistema de saÃºde desde a Iniciativa Bamako de 1987, que promoveu formalmente mÃ©todos baseados na comunidade para aumentar a acessibilidade de medicamentos e serviÃ§os de saÃºde Ã  populaÃ§Ã£o, em parte implementando taxas de usuÃ¡rio. A nova estratÃ©gia aumentou drasticamente a acessibilidade por meio da reforma da saÃºde baseada na comunidade, resultando em uma prestaÃ§Ã£o de serviÃ§os mais eficiente e equitativa. Uma estratÃ©gia abrangente de abordagem foi estendida a todas as Ã¡reas da assistÃªncia mÃ©dica, com subsequente melhoria nos indicadores de assistÃªncia mÃ©dica e melhoria na eficiÃªncia e custo dos serviÃ§os de saÃºde.</td>\n",
       "      <td>A NigÃ©ria estÃ¡ adicionando que tipo de custos ao seu sistema de saÃºde?</td>\n",
       "      <td>{'answer_start': [376], 'text': ['taxas de usuÃ¡rio']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56d130f217492d1400aabbc3</td>\n",
       "      <td>Kanye_West</td>\n",
       "      <td>West foi preso novamente em 14 de novembro de 2008 no hotel Hilton, perto de Gateshead, apÃ³s outra briga envolvendo um fotÃ³grafo do lado de fora da famosa boate Tup Tup Palace em Newcastle upon Tyne. Mais tarde, ele foi libertado \"sem mais aÃ§Ãµes\", segundo um porta-voz da polÃ­cia.</td>\n",
       "      <td>Onde Kanye foi preso pela segunda vez?</td>\n",
       "      <td>{'answer_start': [54], 'text': ['hotel Hilton, perto de Gateshead']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56dd2d7d9a695914005b9530</td>\n",
       "      <td>Prime_minister</td>\n",
       "      <td>Walpole sempre negou que ele era \"primeiro ministro\" e, durante todo o sÃ©culo 18, parlamentares e juristas continuaram negando que qualquer posiÃ§Ã£o desse tipo fosse conhecida pela ConstituiÃ§Ã£o. George II e George III fizeram esforÃ§os Ã¡rduos para recuperar o poder pessoal do monarca, mas a complexidade e as despesas crescentes do governo fizeram com que um ministro que pudesse comandar a lealdade dos Comuns fosse cada vez mais necessÃ¡rio. A longa permanÃªncia do primeiro ministro da guerra William Pitt, o Jovem (1783-1801), combinada com a doenÃ§a mental de George III, consolidou o poder do posto. O tÃ­tulo foi mencionado pela primeira vez em documentos do governo durante a administraÃ§Ã£o de Benjamin Disraeli, mas nÃ£o apareceu na Ordem BritÃ¢nica formal de precedÃªncia atÃ© 1905.</td>\n",
       "      <td>AlÃ©m de Walpole, quem mais negou que o primeiro ministro nÃ£o existisse?</td>\n",
       "      <td>{'answer_start': [82], 'text': ['parlamentares e juristas']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>570c3f1dec8fbc190045be1d</td>\n",
       "      <td>Mary_(mother_of_Jesus)</td>\n",
       "      <td>As doutrinas da AssunÃ§Ã£o ou DormiÃ§Ã£o de Maria estÃ£o relacionadas Ã  sua morte e suposiÃ§Ã£o corporal ao cÃ©u. A Igreja CatÃ³lica Romana definiu dogmaticamente a doutrina da AssunÃ§Ã£o, que foi feita em 1950 pelo Papa Pio XII no Munificentissimus Deus. Se a Virgem Maria morreu ou nÃ£o, nÃ£o Ã© definido dogmaticamente, embora uma referÃªncia Ã  morte de Maria seja feita em Munificentissimus Deus. Na Igreja Ortodoxa Oriental, acredita-se na AssunÃ§Ã£o da Virgem Maria e Ã© celebrada com sua DormiÃ§Ã£o, onde eles acreditam que ela morreu.</td>\n",
       "      <td>Em qual documento papal foi definido o dogma da AssunÃ§Ã£o?</td>\n",
       "      <td>{'answer_start': [221], 'text': ['Munificentissimus Deus']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup environment variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic command `%env` corresponds to `export` in linux. It allows to setup the values of all arguments of the script `run_qa_adapter.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "'n_gpu':n_gpu,\n",
    "'gpu':gpu,\n",
    "'CUDA_VISIBLE_DEVICES':gpu,\n",
    "'model_name_or_path':model_checkpoint,\n",
    "'dataset_name':dataset_name,\n",
    "'train_file':train_file,\n",
    "'validation_file':validation_file,\n",
    "'do_train':do_train,\n",
    "'do_eval':do_eval,\n",
    "'max_train_samples':max_train_samples,\n",
    "'max_val_samples':max_train_samples,\n",
    "'output_dir':output_dir,\n",
    "'overwrite_output_dir':overwrite_output_dir,\n",
    "'max_seq_length':max_seq_length,\n",
    "'pad_to_max_length':pad_to_max_length,\n",
    "'null_score_diff_threshold':null_score_diff_threshold,\n",
    "'doc_stride':doc_stride,\n",
    "'n_best_size':n_best_size,\n",
    "'max_answer_length':max_answer_length,\n",
    "'evaluation_strategy':evaluation_strategy,\n",
    "'per_device_train_batch_size':batch_size,\n",
    "'per_device_eval_batch_size':batch_size,\n",
    "'gradient_accumulation_steps':gradient_accumulation_steps,\n",
    "'learning_rate':learning_rate,\n",
    "'weight_decay':weight_decay,\n",
    "'adam_beta1':adam_beta1,\n",
    "'adam_beta2':adam_beta2,\n",
    "'adam_epsilon':adam_epsilon,\n",
    "'num_train_epochs':num_train_epochs,\n",
    "'warmup_ratio':warmup_ratio,\n",
    "'warmup_steps':warmup_steps,\n",
    "'logging_dir':logging_dir,\n",
    "'logging_strategy':logging_strategy,\n",
    "'logging_first_step':logging_first_step,\n",
    "'logging_steps':logging_steps,\n",
    "'eval_steps':eval_steps,\n",
    "'save_strategy':save_strategy,\n",
    "'save_steps':save_steps,\n",
    "'save_total_limit':save_total_limit,\n",
    "'no_cuda':no_cuda,\n",
    "'seed':seed,\n",
    "'fp16':fp16,\n",
    "'fp16_opt_level':fp16_opt_level,\n",
    "'fp16_backend':fp16_backend,\n",
    "'fp16_full_eval':fp16_full_eval,\n",
    "'disable_tqdm':disable_tqdm,\n",
    "'remove_unused_columns':remove_unused_columns,\n",
    "'load_best_model_at_end':load_best_model_at_end,\n",
    "'metric_for_best_model':metric_for_best_model,\n",
    "'greater_is_better':greater_is_better,\n",
    "'early_stopping_patience':early_stopping_patience,\n",
    "'madx2':madx2,\n",
    "'train_adapter':train_adapter,\n",
    "'adapter_config':adapter_config,\n",
    "'adapter_non_linearity':adapter_non_linearity,\n",
    "'adapter_reduction_factor':adapter_reduction_factor,\n",
    "'language':language,\n",
    "'adapter_composition':adapter_composition\n",
    "}\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    envs['load_lang_adapter']=load_lang_adapter\n",
    "    envs['lang_adapter_config']=lang_adapter_config\n",
    "    envs['lang_adapter_non_linearity']=lang_adapter_non_linearity\n",
    "    envs['lang_adapter_reduction_factor']=lang_adapter_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in envs.items():\n",
    "    %env {k}={v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the output_dir (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can launch the training :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste/uncomment the 2 following lines in the following cell if you want to limit the number of data (useful for testing)\n",
    "# --max_train_samples $max_train_samples \\\n",
    "# --max_val_samples $max_val_samples \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if with_adapters_mlm:\n",
    "    # with lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_checkpoint \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $batch_size \\\n",
    "    --per_device_eval_batch_size $batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition \\\n",
    "    --load_lang_adapter $load_lang_adapter \\\n",
    "    --lang_adapter_config $lang_adapter_config \\\n",
    "    --lang_adapter_non_linearity $lang_adapter_non_linearity \\\n",
    "    --lang_adapter_reduction_factor $lang_adapter_reduction_factor\n",
    "else:\n",
    "    # without lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_checkpoint \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $batch_size \\\n",
    "    --per_device_eval_batch_size $batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "Saving nbest_preds to (...)/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/models_outputs/neuralmind-bert-base-portuguese-cased_squad11pt/qa_lr0.0001_bs16_eps1e-06_epochs10.0_wamlmFalse_madx2True_dsFalse_fp16True_bestTrue_metricf1/output_dir/eval_nbest_predictions.json.\n",
    "\n",
    "[INFO|trainer_pt_utils.py:722] 2021-07-01 19:40:37,300 >> ***** eval metrics *****\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   epoch        =    10.0\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   eval_f1      = 82.0608\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   eval_samples =   10917\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   exact_match  = 69.9054\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3244\r\n",
      "drwxrwxr-x 2 pierre pierre    4096 Jul  1 19:35 .\r\n",
      "drwxrwxr-x 4 pierre pierre    4096 Jul  1 19:37 ..\r\n",
      "-rw-rw-r-- 1 pierre pierre     642 Jul  1 19:35 adapter_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre     240 Jul  1 19:35 head_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre 3294305 Jul  1 19:35 pytorch_adapter.bin\r\n",
      "-rw-rw-r-- 1 pierre pierre    7143 Jul  1 19:35 pytorch_model_head.bin\r\n"
     ]
    }
   ],
   "source": [
    "# folder of the saved task adapter\n",
    "!ls -al {output_dir/task}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6067 (pid 1172553), started 6:53:10 ago. (Use '!kill 1172553' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-de5dcd628dee24b9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-de5dcd628dee24b9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6067;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Application QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true,
    "id": "IJA9CgOBis0F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "id": "NiuPQTxuzRrm"
   },
   "outputs": [],
   "source": [
    "# load the language adapter\n",
    "if with_adapters_mlm:\n",
    "    task_mlm_load_as = 'mlm'\n",
    "    lang_adapter_name = model_qa.load_adapter(\n",
    "        load_lang_adapter,\n",
    "        config=lang_adapter_config,\n",
    "        load_as=task_mlm_load_as,\n",
    "        with_head=False\n",
    "    )\n",
    "else:\n",
    "    lang_adapter_name = None\n",
    "\n",
    "# load adapter\n",
    "if train_adapter:\n",
    "    task_name = 'qa'\n",
    "    load_adapter_qa = str(output_dir/task_name)\n",
    "    adapter_name = model_qa.load_adapter(\n",
    "        load_adapter_qa,\n",
    "        config=adapter_config,\n",
    "        load_as=task_name,\n",
    "        with_head=True\n",
    "    )\n",
    "else:\n",
    "    adapter_name = None\n",
    "    \n",
    "if train_adapter:\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model_qa.set_active_adapters([lang_adapter_name, adapter_name])\n",
    "    else:\n",
    "        model_qa.set_active_adapters([adapter_name])\n",
    "else:\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model_qa.set_active_adapters([lang_adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\n",
    "context = r\"\"\"A pandemia de COVID-19, tambÃ©m conhecida como pandemia de coronavÃ­rus, Ã© uma pandemia em curso de COVID-19, \n",
    "uma doenÃ§a respiratÃ³ria causada pelo coronavÃ­rus da sÃ­ndrome respiratÃ³ria aguda grave 2 (SARS-CoV-2). \n",
    "O vÃ­rus tem origem zoonÃ³tica e o primeiro caso conhecido da doenÃ§a remonta a dezembro de 2019 em Wuhan, na China. \n",
    "Em 20 de janeiro de 2020, a OrganizaÃ§Ã£o Mundial da SaÃºde (OMS) classificou o surto \n",
    "como EmergÃªncia de SaÃºde PÃºblica de Ã‚mbito Internacional e, em 11 de marÃ§o de 2020, como pandemia. \n",
    "Em 18 de junho de 2021, 177 349 274 casos foram confirmados em 192 paÃ­ses e territÃ³rios, \n",
    "com 3 840 181 mortes atribuÃ­das Ã  doenÃ§a, tornando-se uma das pandemias mais mortais da histÃ³ria.\n",
    "Os sintomas de COVID-19 sÃ£o altamente variÃ¡veis, variando de nenhum a doenÃ§as com risco de morte. \n",
    "O vÃ­rus se espalha principalmente pelo ar quando as pessoas estÃ£o perto umas das outras. \n",
    "Ele deixa uma pessoa infectada quando ela respira, tosse, espirra ou fala e entra em outra pessoa pela boca, nariz ou olhos.\n",
    "Ele tambÃ©m pode se espalhar atravÃ©s de superfÃ­cies contaminadas. \n",
    "As pessoas permanecem contagiosas por atÃ© duas semanas e podem espalhar o vÃ­rus mesmo se forem assintomÃ¡ticas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true,
    "id": "t4_ezTchwKZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'dezembro de 2019', score: 0.2859, start: 289, end: 305\n",
      "CPU times: user 20.3 s, sys: 3.14 s, total: 23.5 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quando comeÃ§ou a pandemia de Covid-19 no mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true,
    "id": "a_JgLD8fxdwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'dezembro de 2019', score: 0.2018, start: 289, end: 305\n",
      "CPU times: user 20 s, sys: 4.73 s, total: 24.7 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Qual Ã© a data de inÃ­cio da pandemia Covid-19 em todo o mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true,
    "id": "NecR00-wzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'origem zoonÃ³tica', score: 0.4086, start: 224, end: 240\n",
      "CPU times: user 18.3 s, sys: 4.79 s, total: 23.1 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"A Covid-19 tem algo a ver com animais?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true,
    "id": "RcK4pn1hbLhL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Wuhan, na China', score: 0.701, start: 309, end: 324\n",
      "CPU times: user 23.6 s, sys: 6.27 s, total: 29.9 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Onde foi descoberta a Covid-19?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "id": "7rFEmmsjzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '177 349 274', score: 0.6846, start: 535, end: 546\n",
      "CPU times: user 13.4 s, sys: 2.49 s, total: 15.9 s\n",
      "Wall time: 678 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos casos houve?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true,
    "id": "0v1TTQXDzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '3 840 181', score: 0.8342, start: 605, end: 614\n",
      "CPU times: user 40 s, sys: 9.41 s, total: 49.4 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos mortes?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "id": "f78AggHLzRro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '192', score: 0.5195, start: 574, end: 577\n",
      "CPU times: user 48.2 s, sys: 12.7 s, total: 1min\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos paises tiveram casos?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true,
    "id": "J0AGoVc_xhdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'nenhum a doenÃ§as com risco de morte', score: 0.6356, start: 760, end: 795\n",
      "CPU times: user 40.2 s, sys: 10.4 s, total: 50.5 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quais sÃ£o sintomas de COVID-19\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true,
    "id": "YSQnntVgcHHq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'pelo ar', score: 0.1721, start: 832, end: 839\n",
      "CPU times: user 43.3 s, sys: 11.4 s, total: 54.7 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Como se espalha o vÃ­rus?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
